{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data File**"
      ],
      "metadata": {
        "id": "gqF3bh-3C3aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads the file's contents and saves it to the notebook's files."
      ],
      "metadata": {
        "id": "-CtRKejwC2Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "data_file = \"reddit_comments.csv\"\n",
        "\n",
        "request = requests.get(\"https://drive.google.com/uc?export=download&id=1grbBKQ8SEcujIYSTiKaDbOXbFpuhTijv\")\n",
        "with open(data_file, \"wb\") as file:\n",
        "    file.write(request.content)"
      ],
      "metadata": {
        "id": "1pfFlr0E_i-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine file entries (sanity check)"
      ],
      "metadata": {
        "id": "XtamkdGhDwU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_csv(data_file)\n",
        "print(dataframe.head())"
      ],
      "metadata": {
        "id": "GjNjxukfBMCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Dataset and General Setup**"
      ],
      "metadata": {
        "id": "zPgr8W1gDlM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I primarily use Hugging Face in this project, in order to save my model's weights upon every epoch, I needed a HuggingFace account to push the model to [their model hub](https://huggingface.co/docs/hub/models-the-hub). If you want to replicate my training you will need to create a Hugging Face account and then generate a token that has write access. "
      ],
      "metadata": {
        "id": "gS-WzCgHEFCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You only need to run the `!pip3 install` code blocks once per session."
      ],
      "metadata": {
        "id": "Z3VmlHE3F-X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface_hub"
      ],
      "metadata": {
        "id": "Iw1TxDZ-9u6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste your token into box and follow the instructions. You only need to authenticate once per session."
      ],
      "metadata": {
        "id": "qPZav5yLFKh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "iFS6KohHv8BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers==4.28.0"
      ],
      "metadata": {
        "id": "nKylUvxZ-Cq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Seed\n",
        "\n",
        "You need to run this before the Load and Partition Dataset section to ensure the train, valid and test partitions are the same."
      ],
      "metadata": {
        "id": "CByfOr_jMC6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "RyNpEkasJTkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Partition Dataset"
      ],
      "metadata": {
        "id": "e2Ef5NhsGD20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, Features, ClassLabel, Value\n",
        "\n",
        "dataset = load_dataset('csv', data_files=data_file, split=\"train\", download_mode=\"reuse_cache_if_exists\")\n",
        "\n",
        "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "\n",
        "final_dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})\n",
        "\n",
        "print(final_dataset)"
      ],
      "metadata": {
        "id": "carSUqGvxQDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dataset_divide(dataset_partition, partition_name):\n",
        "  count_pos = 0\n",
        "  count_neg = 0\n",
        "  for entry in dataset_partition:\n",
        "    if entry[\"label\"] == 0:\n",
        "      count_neg += 1\n",
        "    else: #entry[\"label\"] == 1:\n",
        "      count_pos += 1\n",
        "  print(\"partition_name:\", partition_name)\n",
        "  print(\"poitive_count: \", count_pos)\n",
        "  print(\"negative_count: \", count_neg)"
      ],
      "metadata": {
        "id": "ifbq3djfxW4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine Final Dataset"
      ],
      "metadata": {
        "id": "ff6T1U_uNAvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_dataset_divide(final_dataset[\"train\"], \"Train\")\n",
        "print(final_dataset[\"train\"][0], \"\\n\")\n",
        "count_dataset_divide(final_dataset[\"test\"], \"Test\")\n",
        "print(final_dataset[\"test\"][0], \"\\n\")\n",
        "count_dataset_divide(final_dataset[\"valid\"], \"Valid\")\n",
        "print(final_dataset[\"valid\"][0], \"\\n\")"
      ],
      "metadata": {
        "id": "35py8ENBz6iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "wab5IkehwZEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU/CPU"
      ],
      "metadata": {
        "id": "6XKdvdujLo1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "5_1tDLNIbBqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "task = \"sst2\"\n",
        "num_labels = 2\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "metric = load_metric('glue', 'sst2')\n",
        "\n",
        "batch_size = 32\n",
        "learn_rate = 5e-6\n",
        "num_epochs = 6\n",
        "w_decay = 0.01"
      ],
      "metadata": {
        "id": "02gAfrUjS94-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer"
      ],
      "metadata": {
        "id": "m0J23tJtQHCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "metadata": {
        "id": "oqDa8YAWD2vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Dataset"
      ],
      "metadata": {
        "id": "FLDv6BrMQaD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(entries):\n",
        "    return tokenizer(entries[\"comment\"], truncation=True)\n",
        "\n",
        "encoded_dataset = final_dataset.map(preprocess_function, batched=True, load_from_cache_file=False)"
      ],
      "metadata": {
        "id": "T4qoIyuvIezp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "UAxoXq-XQhIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "cdeq49mYVbpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Arguments\n",
        "\n",
        "`model_name` is the name that the trainer will be saved as on the Hugging Face Hub"
      ],
      "metadata": {
        "id": "R6fgxrppehTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"reddit-comment-sentiment-final\"\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    model_name,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=learn_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=w_decay,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "TQaYiTNRVlOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Metrics - Accuracy"
      ],
      "metadata": {
        "id": "CBOpNYANQwqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if task != \"stsb\":\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "    else:\n",
        "        predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "6DeMVaQYcVGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "gfCYA6ydejn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    train_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "A0mc4K4icskZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "6Eu8TeuWdL9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print evaluation on validation set, should be the Trainer version with the best accuracy (sanity check)"
      ],
      "metadata": {
        "id": "hzBufgI1wyn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fF2Hzq3sTQUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push Trainer to the Hugging Face Hub"
      ],
      "metadata": {
        "id": "XHQuBEzzx8Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "En7HCvZNdGvp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}